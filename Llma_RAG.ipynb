{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG System Using Gemena With Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY '] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.1.0\n"
     ]
    }
   ],
   "source": [
    "### Install necessary package\n",
    "! pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### install langchain\n",
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting install\n",
      "  Downloading install-1.3.5-py3-none-any.whl.metadata (925 bytes)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (4.38.2)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (0.21.4)\n",
      "Requirement already satisfied: Pillow in /opt/mamba/lib/python3.11/site-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.99)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/mamba/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: install, sentence_transformers\n",
      "Successfully installed install-1.3.5 sentence_transformers-2.5.1\n"
     ]
    }
   ],
   "source": [
    "### embedding with sentence transformer\n",
    "!pip install install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /opt/mamba/lib/python3.11/site-packages (0.10.18)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.8)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.18 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.10.18.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.9)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /opt/mamba/lib/python3.11/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma<0.2.0,>=0.1.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.1.6)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/mamba/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.18->llama_index) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (1.13.3)\n",
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.18->llama_index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /opt/mamba/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /opt/mamba/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (1.23.26)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/mamba/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.4.0,>=0.3.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index) (0.3.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama_index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama_index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/mamba/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/mamba/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.16.0)\n",
      "Requirement already satisfied: chromadb<0.5.0,>=0.4.22 in /opt/mamba/lib/python3.11/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.4.24)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/mamba/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.18->llama_index) (2.6.3)\n",
      "Requirement already satisfied: anyio in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama_index) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama_index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.0.4)\n",
      "Requirement already satisfied: idna in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama_index) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/mamba/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.18->llama_index) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/mamba/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.8.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in /opt/mamba/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (1.23.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.18->llama_index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/mamba/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.18->llama_index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/mamba/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/mamba/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.18->llama_index) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama_index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama_index) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama_index) (2024.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.1.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/mamba/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.28.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.5.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (6.1.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/mamba/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.9.15)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/mamba/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.18->llama_index) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.18->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/mamba/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.18->llama_index) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.18->llama_index) (1.16.0)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/mamba/lib/python3.11/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.0.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /opt/mamba/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.36.3)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/mamba/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (2.28.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/mamba/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/mamba/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/mamba/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.2.2)\n",
      "Requirement already satisfied: coloredlogs in /opt/mamba/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/mamba/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /opt/mamba/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.25.2)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.12)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (6.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.62.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (68.2.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/mamba/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/mamba/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/mamba/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (2.2.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/mamba/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.21.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/mamba/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/mamba/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/mamba/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/mamba/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/mamba/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/mamba/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/mamba/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.13.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/mamba/lib/python3.11/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.17.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/mamba/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/mamba/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface\n",
      "  Downloading llama_index_llms_huggingface-0.1.3-py3-none-any.whl.metadata (792 bytes)\n",
      "Collecting huggingface-hub<0.21.0,>=0.20.3 (from llama-index-llms-huggingface)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-llms-huggingface) (0.10.18.post1)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /opt/mamba/lib/python3.11/site-packages (from llama-index-llms-huggingface) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.38.2)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (23.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/mamba/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.13.3)\n",
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (10.2.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/mamba/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.99)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/mamba/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.27.2)\n",
      "Requirement already satisfied: psutil in /opt/mamba/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/mamba/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/mamba/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.6.3)\n",
      "Requirement already satisfied: anyio in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.4)\n",
      "Requirement already satisfied: idna in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/mamba/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/mamba/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/mamba/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/mamba/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/mamba/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/mamba/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
      "Downloading llama_index_llms_huggingface-0.1.3-py3-none-any.whl (7.2 kB)\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, llama-index-llms-huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.4\n",
      "    Uninstalling huggingface-hub-0.21.4:\n",
      "      Successfully uninstalled huggingface-hub-0.21.4\n",
      "Successfully installed huggingface-hub-0.20.3 llama-index-llms-huggingface-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='a4b469f8-ab5e-4805-b751-ce7f7af37b7c', embedding=None, metadata={'page_label': '1', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='École nationale de la statistique\\net de l’administration économique\\nBusiness data challenge report - group 4\\nPrediction of a property price\\nStudents :\\nGregoire HUBERT\\nNicolas JULIEN\\nNick jofrein TEDONZE\\nSalah-Eddine EL MOUSLIH\\nAxelNAOARINE\\nApril 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42ac80da-7671-42c1-8bda-3301dfbebfc4', embedding=None, metadata={'page_label': '1', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents\\n1 Introduction 3\\n1.1 Presentation of the project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.1.1 Meilleurtaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.1.2 Project objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.1.3 Constraints & Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.1.4 Proposed Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Literature review 6\\n3 Data 8\\n3.1 Presentation and preparation of dvf database . . . . . . . . . . . . . . . . . . . . . 8\\n3.1.1 The “demandes des valeurs foncières” database . . . . . . . . . . . . . . . . . 8\\n3.1.2 Preparation of dvf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n3.2 Open databases used for the project and integration . . . . . . . . . . . . . . . . . . 12\\n3.2.1 Dataset Enrichissement : Iris Database and Equipments . . . . . . . . . . . 12\\n3.2.2 Construction of a new variable: the average price per m2of the 10 nearest\\nproperties, for each property . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.3 Stat Desc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.3.1 Correlation of explanatory variables with target variable . . . . . . . . . . . 15\\n3.3.2 Study of IRIS variables, the issue of metropolises and the link with price per\\nm2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.3.3 Study of types of properties (houses and apartments) . . . . . . . . . . . . . 18\\n3.3.4 Study of target variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 Modeling 21\\n4.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.1.1 The working features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.1.2 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.2 Split Train and test and Data leakage problem . . . . . . . . . . . . . . . . . . . . . 22\\n4.3 The models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.3.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.3.2 The use of ensemble learning models to limit over-fitting . . . . . . . . . . . 24\\n4.4 Selection of the best parameters of models by cross-validation and selection of the\\nbest models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.4.1 K-fold Cross Validation and scoring Metrics . . . . . . . . . . . . . . . . . . 27\\n4.4.2 Choice of the best model by type of property and metropole . . . . . . . . . 28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1eecf72b-7780-4a4e-bce7-43dc36480c8a', embedding=None, metadata={'page_label': '1', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5 Results 29\\n5.1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5.1.1 Metrics used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5.1.2 Performance assessment of the models on the test dataset . . . . . . . . . . . 30\\n5.2 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2.1 Feature importance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2.2 Understanding of errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n5.3 Robustness of models over time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\nConclusion 37\\nBibliography 38\\nAppendix 39', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='17f2c433-1b44-437d-a06d-1dc6ada1e3ee', embedding=None, metadata={'page_label': '2', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='List of Figures\\n1 Summary of multiple transaction cleaning . . . . . . . . . . . . . . . . . . . . . . . 9\\n2 Share of properties droped by filtering on houses and apartments . . . . . . . . . . . 10\\n3 Example of equipements in 18th arrondissement of Paris . . . . . . . . . . . . . . . 13\\n4 Example of the 10 nearest sold properties for the target property located at 1 Rue\\nBossuet, 69006 Lyon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n5 Correlation between prix_m2 andprix_m2_zone in each IRIS . . . . . . . . . . . . 15\\n6 Correlationbetweeneachexplanatoryvariableandthetargetvariable( prix_m2_actualise ) 16\\n9 Distribution of surface and number of rooms by type of properties . . . . . . . . . . 18\\n10 Share of each properties in all metropoles . . . . . . . . . . . . . . . . . . . . . . . . 19\\n11 Distribution of price m2for each métropoles and house/apartments . . . . . . . . . 20\\n12 Example of a decision tree for a property . . . . . . . . . . . . . . . . . . . . . . . . 26\\n13 Differences in operation between ensemble learning models . . . . . . . . . . . . . . 26\\n14 Feature Importance of the Bordeaux’ apartments model . . . . . . . . . . . . . . . . 32\\n15 Feature Importance of the Bordeaux’ houses model . . . . . . . . . . . . . . . . . . 33\\n16 Histogram of errors made by the Nice-Apartment model . . . . . . . . . . . . . . . 34\\n19 Feature correlation with prix_ m2_actualise Grand Paris Métropole . . . . . . . . . 39\\n20 Feature correlation with prix_ m2_actualise Rennes Métropole . . . . . . . . . . . . 40\\n21 Distribution of surface and number of rooms by type of properties, by metropole . . 41\\n22 Documentation page of ’Discount’ module . . . . . . . . . . . . . . . . . . . . . . . 42\\n23 Documentation page of ’Core’ module . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b8a5762-872d-4ce7-9a89-182dca1017f5', embedding=None, metadata={'page_label': '3', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Introduction\\n1.1 Presentation of the project\\n1.1.1 Meilleurtaux\\nMeilleurtaux is a financial product brokerage company created in 1999 by Christophe Crémer.\\nInitially present only online, Meilleurtaux widened its mode of distribution by creating a network\\nof agencies from the 2000’s. Currently, there are more than 300 of them all over France.\\nThe company’s objective is to advise individuals looking for financial services. Its goal is to put\\nthem in touch with the banking establishment offering them the best solutions in view of their\\nprofile and their project. Initially specialized in real estate loans, Meilleurtaux has over the years\\nexpanded its offer to other loans (e.g.: consumer), insurance, banking and savings.\\nIn order to achieve this objective, Meilleurtaux offers a brokerage service, online comparison as\\nwell as a multitude of simulation tools directly accessible from its website to provide the as many\\ninformation as possible to its customers.\\nInitially online only, Meilleurtaux keeps on with this strategy and seeks to be always more digital\\nfor an always more frictionless user experience. This is reflected through different axes:\\n•Explore data to give 360 degrees economic insights to customers and community.\\n•Build tools which bring intelligence to customers and staff.\\n•Build a data-ops ecosystem for all the group: datalake, shared AI and machine learning\\nbased tools, develops data science platform, API.\\n1.1.2 Project objective\\nIt is in this context that this project takes place. The goal of the Business Data Challenge is\\nto provide a tool for predicting the price of real estate according to its characteristics (type of\\nproperty, location, surface area,...). This tool made available to customers of Meilleurtaux would\\nallow them to estimate the real estate value of their dwelling. Thus, this estimate will be used in\\ntheir profile in order to seek the best credits that could be proposed to them according to their\\nproject. This estimate is useful in two cases:\\n•In a mortgage loan project, when you need a bridging loan.\\n•In a credit consolidation project, when you have a mortgage guarantee.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d778f07b-0537-4aac-9af1-9353ed60e901', embedding=None, metadata={'page_label': '4', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.1.3 Constraints & Challenges\\nIn our opinion, this project presents 4 major difficulties to overcome:\\n•First, the subject contains a significant temporal component . Indeed, the price of a property\\ncan vary in time and the estimate of a property does not have a unique estimate depending\\non the time t when it is evaluated. Therefore, the prediction error actually contains two\\ncomponents :\\n1) The hedonic prediction error, i.e. how far we are from the correct prediction without\\ntaking into account the temporal aspect, which means that we misunderstand how the char-\\nacteristics of the property influence its price\\n2) The prediction error of the price index, i.e. the extent to which the temporal variation\\nof the price of the property has been wrongly predicted and therefore that the prediction of\\nthe global index of real estate prices at a given time is wrong.\\nHowever, it is rather likely that Meilleurtaux is looking for a model that can minimize the\\nfirst error, that is to say to determine the components that determine the price of a property\\nrather than to have a model that just efficiently predicts real estate prices globally, which\\ncorresponds to two very different tasks. How to obtain a model that minimizes only the\\nnon-temporal estimation error ?\\n•Thistemporaldimensionoftheproblemcanalsoleadtoaleakage problem when building our\\nmodel. Indeed, when we manipulate a dataset, we have access to the whole price history. We\\nhave to be careful about the methodology used, especially in the construction of features,\\nwhich may contain information about the future. Typically, it would be possible to obtain\\nthe price index for each year and thus obtain a very powerful model to predict what the price\\nwould have been at a past date. But the objective of Meilleurtaux is to obtain a model with\\ngood performance in inferability. In practice, it is only possible to obtain past data, and\\nfuture data can only be predicted at best. How to avoid the leakage problem and be sure\\nthat the model remains effective when only present and past information are available ?\\n•In addition to the temporal dimension, the subject also contains a significant geographical\\ndimension . It is certain that the geo-location of a property will make its price varyating, and\\nthat many geographic features can be constructed. Which ones to select and why? Also, the\\nFrench territory is very heterogeneous concerning real estate and is actually made up of many\\nsub-markets. The economic reality and the factors of variations will be very different for a\\nproperty located in Nice and a property located in Paris. How to manage this heterogeneity\\nand the differences between the real estate markets ?\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='960799bd-ffd2-470f-9ed2-5237f43f9b71', embedding=None, metadata={'page_label': '5', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='•Finally, from a more practical point of view, the subject also presents an important stake\\nof data comprehension and cleaning . Indeed, the data presented are very large and archaic,\\ntherefore, the appropriation of the information contained in the dataset, the feature selection,\\nthe management of missing values and outliers, the reinforcement mergers, are going to be\\nvery important steps with high added value for finding the best solution. There is no point in\\nlooking for the best model if the data has not been intelligently cleaned up beforehand. How\\ncan we best pre-process our data to get the maximum information out of it while limiting\\nerrors ?\\n1.1.4 Proposed Methodology\\nWe chose an approach that aimes to address all of the above points as best as possible.\\nThus, our first strong choice is to actualize the prices of our entire training dataset. By actu-\\nalizing the prices, we ensure that each row of our dataset contains a price that is not impacted by\\nthe temporal dimension of the problem. Therefore, by training a model on that data, it minimizes\\nthe hedonic error mentioned above, which is the one that interests Meilleurtaux. The problem\\nwith this methodology is that it leads to data leakage: the discount is calculated using the variable\\nof interest, the price of a property, and therefore a small part of the information carried by this\\nvariable is injected directly into the feature, which can bias the model.\\nIt was therefore important to keep a part of the dataset not actualized in order to test the inference\\ncapacities of the model and to see how it performs when the prices are not updated, which is the\\ncase in real conditions. We then perform a \"split\" of the data by keeping the sales after the 2nd\\nquarter 2021 undiscounted in order to obtain performance indicators closer to reality on our model.\\nFor the geographical dimension, we decided to enrich the dataset by using different information\\nto create new features containing information on the local environment of the properties. Indeed,\\nin the original dataset, the rows contain only information directly related to the property, while\\nin reality, the neighborhood can also have a strong impact on the price.\\nFinally, we decided to create different models that would be specifically trained on markets/types\\nof property. Indeed, the economic realities that modulate the different markets can be too complex\\nto understand for a single model, so we manually split up to create specialized models. It doesn’t\\nbother Meilleurtaux to have one model per city, and the only cost of this method is that it can\\nadd complexity to the code and increase computation time. So we limited ourselves to 20 models:\\none model for each metropolis in the 10 most represented in our dataset - Paris, Marseille, Lyon,\\nToulouse, Bordeaux, Lille, Montpellier, Nice, Nantes, Rennes; and for each major property type -\\nhouse or apartement.\\nFor the rest of the work, we adopt a \"classical\" approach of machine learning projects.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5bae7ac-3da4-43c6-85eb-3a8ab2539903', embedding=None, metadata={'page_label': '6', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 Literature review\\nThe subject of estimating and predicting the price of a property is the source of many studies.\\nInitially these studies were mainly based on econometric methods. However, in recent years, the\\nuse of machine learning methods has been included in an increasing number of studies. Moreover,\\nwe notice that this subject is universal. It can be found in a multitude of countries, each studying\\nthe real estate market in its own country but with similar results.\\nIn the different studies, two types of data can be distinguished. The first is the data relating\\nto the characteristics of the property. They can be : the surface, the number of rooms, the age\\nof the property or in certain cases more precise data on the number of rooms according to their\\nfunction (number of bathrooms, bedrooms, living rooms,...). These property-specific variables are\\neffectively the most natural features to use to predict the price of a property. Nevertheless, the\\ndifferent studies agree on using variables outside the property that characterize the neighborhood.\\nThesevariablesvaryfromonestudytoanotherdependingontheavailabilityofdata. Theyinclude:\\n•The social characteristics of the neighbor-\\nhood\\n•Distance to public transport\\n•Accessibility to work\\n•The area in which the property is located\\n(municipality, neighborhood,...)\\n•The population density in the area•Type of street (to assess the view from\\nthe property)\\n•Quality of the school\\n•The facilities in the vicinity\\n•Level of crime\\n•Noise level\\nIn the case of France, the studies aggregate these variables on the IRIS scale, which is the basis\\nfor the dissemination of sub-municipal statistics by INSEE. The authors choose to use the loga-\\nrithm of the price per square meter as the dependent variable. Taking log makes the model more\\nrobust for prediction purposes, and the estimates become less vulnerable to extreme observations.\\nIn the case of modeling, different methods are used depending on the purpose of the study. In\\nthe analysis of the influence of explanatory variables on the price of the property, the studies agree\\non using the hedonic regression model described by Rosen in 1974. Its operation is based on a\\nvector of attributes, which may be empty or consist of a set of features, which is assigned to each\\ncharacteristic or group of characteristics. Hedonic models can accommodate non-linearity, inter-\\nactions between variables, or other complex evaluation situations. This is why they are regularly\\nused in problems of estimating the effect of characteristics on real estate prices.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a202fab8-0184-43e9-a0ff-a9e419084a6d', embedding=None, metadata={'page_label': '7', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In the case where researchers are interested in predicting the price of the property, they favor the\\nuse of machine learning methods. In all of these, ensemble learning models, bagging or boosting\\nget the best results. The random forest and Xgboost thus appear to be the preferred models for\\npredicting the price of a property in view of the state of the art. These methods have the advantage\\nof limiting overfitting, having good inferability and correctly capturing geographical effects. So,\\nthese are the same methods that will be studied in this project.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99de9299-dec5-49d8-bf8c-328a1c7d5e51', embedding=None, metadata={'page_label': '8', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3 Data\\n3.1 Presentation and preparation of dvf database\\n3.1.1 The “demandes des valeurs foncières” database\\nThedatabase“demandesdevaleursfoncières” (dvf)gathersinformationonpropertyvaluesdeclared\\nat the time of real estate transfers. The main information contained in the database is the value\\nof the property, the address, the precise location (latitude, longitude), the surface_reelle_bati and\\nthe surface area of the land, the nombre_pieces_principales the type of properties (outbuilding,\\nhouse, apartment and commercial or industrial properties. dfv database includes transactions from\\n2017 to june 2022. Some columns have a lot of missing data. These include the “lot” columns.\\nFor this category of columns it does not necessarily mean that data are actually missing but this\\ncomes from the definition of “lot”. We will come back to their usefulness and their treatment in\\nthe part of preparation of the dvf table. Missing data are not a problem for most of the columns\\nbecause they are not interesting ( nature_culture ,ancien_nom_commune , ...). Missing data are\\nmore of an issue for some other columns. This is notably the case for surface_reelle_bati and\\nlatitude-longitude variables. As we only miss about 1% of localisation variable and the fact that\\nthey are indispensable for our project, we choose to drop the corresponding observations. We will\\nneed to have a more precise look at surface_reelle_bat variable in the preparation step.\\n3.1.2 Preparation of dvf\\nThe first step is to clean the dvf database. This cleaning follows several steps. After each step, we\\nmonitor the state of our database: quantity of deleted properties, and this metropole by metropole\\nto make sure we keep enough data for the estimation.\\nRecovery of the most represented cities in the dvf database\\nWe focus all our future work on estimating the prices of properties located in the most im-\\nportant zones. The zones are defined as the “Établissements publics de coopération intercommu-\\nnales” (EPCI). We have retained the 10 EPCIs most represented in the dvf database: Bordeaux\\nMétropole, Montpellier Méditerranée Métropole, Métropole Européenne de Lille, Métropole Nice\\nCôte d’Azur, Métropole d’Aix-Marseille-Provence, Métropole de Lyon, Métropole du Grand Paris,\\nNantes Métropole, Rennes Métropole, Toulouse Métropole. These metropolises account for 18%\\nof total transactions, before filtering on multiple properties and property types.\\nWe choose to focus on a few zones after having discussed this point with Meilleurtaux and DataS-\\ntorm. This choice is based on two points. First, it would be difficult to estimate the price of\\nproperties that are scattered across the territory. Most of the useful information for the estimation\\ntask are geographical ones and requires a minimum density of known transactions. This density\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91db8a9f-50ec-4f06-800f-35aa3b57f0b0', embedding=None, metadata={'page_label': '9', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='is mostly found in the biggest metropoles. Second, the goal of the project is to estimate the real\\nestate price. Getting a good score in some important areas is more interesting for Meilleurtaux\\nthan struggling with low density areas.\\nCleaning up multiple transactions\\nWe only keep transactions involving houses and apartments. Other types of properties are not\\nconcerned by the Meilleurtaux project. To handle the DVF database it remains indispensable to\\nclean, filter, and enhance our database with new features. Indeed , one of the first main goal is\\nto deal with complex mutations. We have decided to work with mutations which include only one\\nreal estate based on the field “numero de disposition”. To do that , we identify a unique mutation\\nas the concatenation of id_mutation anddate_mutation . We summarize our analysis with the\\nfollowing graph:\\nFigure 1: Summary of multiple transaction cleaning\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dc5cee6b-5c90-4878-b355-b25c2c598a52', embedding=None, metadata={'page_label': '10', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='We are left with 7% of the original dataset and we have droped 61% of the transactions of the\\nprevious dataset (after selction of metropolisis).\\nOnce this transformation done, we can evaluate the rate of suppression in our dataset after having\\nfiltered our data on nature_mutation sales and type_local Appartement and Maison.\\n(a) Share of properties droped by filtering\\non houses\\n(b) Share of properties droped by filtering\\non apartments\\nFigure 2: Share of properties droped by filtering on houses and apartments\\nPrice actualisation\\nAs previously explained, temporality is an important aspect in this project. The working database\\ngathers the sales of real estate in France since 2017. However, all characteristics being equal, it\\nis not possible to compare a price per m2of a property sold in 2017 to a property sold in 2021.\\nIn fact, due to inflation and the evolution of the real estate market over time, the price scale can\\nvary greatly. This is why the model will be trained on a database with prices corrected for this\\ntemporality. Thus, thanks to updated prices, all the properties in the database behave as if they\\nhad all been sold simultaneously. In this way, the model will predict the price of a property on the\\ndate of the update.\\nTo update the prices, we use the real estate price index provided each quarter by the French\\nNational Institute of Statistics and Economic Studies (INSEE) in association with the Institute of\\nNotaries. This one evaluates the evolution of the average real estate price through an index based\\non a base 100 in 2015. In order to make the update more precise, it is decided to update the prices\\naccording to the real estate zone in which the commune is located. Indeed, each French commune\\nis classified in 5 real estate zones taking into account its attractiveness, its density, its population\\nand other qualitative criteria. In addition, to improve the accuracy of the update on large cities\\n(cities on which this project is focused) we decided to use the index provided for them. 4 index\\nare available to us:\\n•Paris - apartments\\n•Agglomeration of Marseille - apartments\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b32b868-5524-4993-aa57-22672474d4f1', embedding=None, metadata={'page_label': '11', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='•Agglomeration of Lyon - apartments\\n•Agglomeration of Lille - houses\\nIn order to make the update more precise, we decide to do it by type of property, house or apart-\\nment. However, forlargecities, onlyonetypeofpropertyindexisprovided. So, weusethenational\\nindex for houses and apartments in order to estimate for these 4 cities, the type of property index\\nnot present in the initial database.\\nFinally, to update the price, we calculate the percentage change between the index of the\\nquarter of sale and the index of the quarter of update. From this calculation, a price discount\\ncoefficient is applied to the price of the property value.\\nGeneral filters on the dvf base\\nAfter having carried out these first cleanings aiming at keeping only the transactions con-\\ncerned by the project (metropoles, house/apartment, multisales filter), we engage the filtering\\nstage on the estimation variables and the target variable. The dvf database is subject to outliers,\\nprobably due to data entry errors. We therefore set up hard filters on the variables concerned:\\nsurface_reelle_bati andnombre_pieces_principales .\\nAfter studying the properties metropole by metropole and distinguishing the types of properties\\nhouse/apartment, we choose different values for houses and apartments. The thresholds chosen\\nmatch the 0.99 quantile for each properties type and metropole. We could have chosen thresholds\\nspecific to each metropole but they are quite homogeneous and the goal of this step is only to drop\\nthe inconsistent values in a general meaning, not a metropole specific one. Thus, for the rest of the\\nproject, we exclude outliers or exceptional properties of more than 360 m2or more than 10 main\\nrooms for houses and of more than 200 m2or more than 6 main rooms for apartments. We are\\nleft with 6% of the original dataset and we have droped 18% of the transactions of the previous\\ndataset (after filtering on multivente and type_local ).\\nAfter having carried out the two steps of price update and split test train (step 3), we filter the\\nproperties on the basis of their price. Special attention must be paid to the problem of data leak-\\nage. Indeed, only train data must be used here to calculate the filter thresholds on these same\\ntrain data. The price used for the train data is the discounted price as defined in section 3. For\\ntest data, we use the same quantile threshold as calculated on train data. This ensures that we do\\nnot use test data in our preprocessing steps. We are left with 6% of the original dataset and we\\nhave droped 3% of the transactions of the previous dataset (after filtering on surface_reelle_bati\\nandnombre_pieces_principales ).\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88362432-975a-45dc-9804-2ec1e3922135', embedding=None, metadata={'page_label': '12', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Open databases used for the project and integration\\n3.2.1 Dataset Enrichissement : Iris Database and Equipments\\nIn order to further enrich the dataset and include data concerning the neighbourhood of the\\nproperty, information called IRIS is used. The IRIS code allows the French territory to be broken\\ndown into small homogeneous areas in terms of population or level of activity, at a finer scale than\\nthe simple commune code. The IRIS can be divided according to different criteria, there are 3\\nmain ones according to the INSEE website:\\n•housing IRIS: their population is generally between 1,800 and 5,000 inhabitants. They are\\nhomogeneous in terms of the type of housing and their boundaries are based on the major\\nbreaks in the urban fabric (railways, waterways, etc.)\\n•Activity IRISs: they group together approximately 1,000 employees and have at least twice\\nas many salaried jobs as the resident population\\n•miscellaneous IRIS - large specific areas with few inhabitants and a large surface area (leisure\\nparks, port areas, forests, etc.).\\nThus, the IRIS code corresponds to a kind of sub-municipality. It is then possible to merge the\\nIRIS database with our dataset in order to give each dwelling an IRIS code using its geographical\\ncoordinates. For some IRIS the database is incomplete. We have chosen an analogue methodology\\nto deal with these missing data. For all properties that miss an information we retrieve the in-\\nformation from its closest neighbor. This is a rigorous approach as the area we consider are quite\\ndense and two close properties, despite not being in he same IRIS, will have very close values for\\nthe IRIS variables.\\nThis IRIS code can then be used to link the dwelling to other databases such as the INSEE’s\\npermanent equipment base (bpe).\\nIt is constructed from various administrative sources and lists a wide range of facilities and\\nservices, market or not, accessible to the public throughout France on 1 January of each year, in\\neach IRIS. rq is what we do by year?\\nThus, for each dwelling, we can construct features that list the number of facilities available\\nin its IRIS. As the database contains 188 types of services, we decided to select the most relevant\\nones and to group them into large categories:\\n•Banks: Number of banks, savings banks, insurance companies in the IRIS...\\n•Post office\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='249c7197-d0ab-4ce2-a5a9-5e54f4954468', embedding=None, metadata={'page_label': '13', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='•Shops: Includes the categories \"Hypermarket, Supermarket, DIY superstore, Supermarket,\\nGrocery, Bakery, Butcher’s, Deli, Frozen food, Fish shop\" of the BPE.\\n•First Cycle Schools: Aggregates nursery and elementary schools\\n•Second Cycle Schools: Includes all middle and high schools\\n•General Practitioners\\n•Railway stations\\n•Cinemas\\n•Libraries\\n•Remarkable spaces and heritage\\nThese categories seemed to be the most important for us in understanding the quality of the\\nproperty. We preferred not to include all the facilities to avoid having too many features, which\\ncould cause overfitting.\\nFigure 3: Example of equipements in 18th arrondissement of Paris\\n3.2.2 Construction of a new variable: the average price per m2of the 10 nearest\\nproperties, for each property\\nAfter having carried out the two steps of price updating and train-split (step 3), we construct\\na new variable, the average price per m2of the 10 nearest properties, for each property. Each\\nproperty in the dvf database is therefore assigned a new variable. The construction of this variable\\nis motivated by the structure of the property markets. Prices per m2are largely determined by\\nthe location of the property. Thus, knowing the average price per m2in the area of the property\\nwhose price per m2is to be estimated should be a good basis.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='397cbb1a-2fd3-432f-86b5-5247d694bbf6', embedding=None, metadata={'page_label': '14', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 4: Example of the 10 nearest sold properties for the target property located at 1 Rue\\nBossuet, 69006 Lyon\\nOthercriteriacaninfluencethepriceper m2ofaproperty,suchasthe nombre_pieces_principales\\nand the total area. As properties that are close to each other may have different characteristics\\non these two variables, the price per m2of the area may be distorted. In order to eliminate the\\ninfluence of these two criteria as much as possible, and thus to recover only the ’true’ price per m2,\\nwe have set up a second method. This is also based on the 10 nearest properties. However, instead\\nof simply calculating the average of the prices per m2, we run a regression of the price per m2on\\nthese two variables and on the data of the 10 closest properties. The intercept of this regression\\nshould represent the price per m2smoothed for the effects of the nombre_pieces_principales and\\nthe total area. It is therefore included as a new variable for each property. When predicting on\\nfinal test data or in the actual use of the tool, the 10 nearest neighbors of the property whose price\\nperm2is to be predicted will be searched in the historical dvf database.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d32cf08-b8b4-4425-888f-4222858232f0', embedding=None, metadata={'page_label': '15', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 5: Correlation between prix_m2 andprix_m2_zone in each IRIS\\nThese graphs also show the spatial correlation in each city regarding prices per m2: in each\\nIRIS, to what extent does the price of neighbours have an impact on my own price ?\\nNote: we decided to aggregate the m2prices of neighbours with this method in order to obtain\\na single indicator on the m2price of neighbours. We could also have included directly in feature\\nprice_m2_nearest1 ,price_m2_nearest2 , etc. and let the model find itself the optimal calculation\\n(be it an average, a median, a quantile, or an even more complex formula) to best combine the\\nneighbours’ prices. However, this represented too great a risk of overfitting, with variables that are\\nhighly correlated to our Y, but whose actual contained information is less clear and interpretable\\nthan a single index.\\n3.3 Stat Desc\\n3.3.1 Correlation of explanatory variables with target variable\\nWe start by studying the correlation of each of our explanatory variables with our target variable,\\nthe discounted price per m2.\\nWe can observe that the average price per m2in the area is the most correlated variable with\\nour target variable. Next come the income variables in the IRIS (7th, 6th, 8th, 9th deciles) as well\\nas the third quartile of income in the IRIS. The median income is also highly correlated. Some\\nIRIS variables related to poverty are also highly negatively correlated: share of housing benefits in\\nIRIS income in particular. The variables specific to each of the properties are not very correlated\\n(surface area and number of rooms).\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89c85087-7d5c-4cde-bb5f-9f718225fbd0', embedding=None, metadata={'page_label': '16', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 6: Correlation between each explanatory variable and the target variable\\n(prix_m2_actualise )\\nThe levels of correlation and the order of the most correlated variables is different in each\\nmetropolis. Thisobservationfeedsourmethodofamodelpermetropolisandperhouse/apartment.\\nFor example, the correlations are much weaker for the metropolis of Rennes than for Paris. More-\\nover, the order is also very different for these two metropolises (see figure 19 & figure 20 in\\nappendix). This observation is also true for the other metropolises. We will go into more detail in\\nthe study of these variables, starting with the environment variables.\\n3.3.2 Study of IRIS variables, the issue of metropolises and the link with price per\\nm2\\nAs we have seen, many of the IRIS variables are highly correlated with our target variable. Let\\nus study them in more detail and explain why they are so. We take the example of four variables:\\nthe median income by IRIS, the inter-quartile range relative to the median, the seventh decile and\\nthe share of housing benefits.\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70d6dafd-243f-45ab-86ab-5283f26722d6', embedding=None, metadata={'page_label': '17', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Median revenu by IRIS and prix au m2\\nWe can observe on these two maps of Lyon and Paris (intra muros for reasons of legibility) that\\nthe median income of the IRIS (background color) does indeed seem to be positively correlated\\nwith the average price per m2of the properties in the IRIS (circles).\\n(a) Median revenu by iris and average price m2in\\neach IRIS, Lyon\\n(b) Median revenu by iris and average price m2in\\neach IRIS, Paris\\nInterquartile range to median and price per m2\\nThe interquartile range is a measure of income inequality within the IRIS. We can observe that\\nthe most unequal neighbourhoods are the neighbourhoods where the richest people live, notably\\nthe 16th arrondissement of Paris. The inequality here is accentuated by the fact that household\\nemployees often live in the same neighbourhood and therefore have a strong influence on local\\ninequalities.\\nThe strong correlation between the interquartile range and the price per m2must rather be due\\nto relatively homogeneous neighbourhoods, where inequalities are rather low because all incomes\\nare low on average. A low inequality is therefore very much linked to a low price per m2. This\\nseems to be well verified in Paris.\\nHowever, in Nice (left map) strong inequalities are also linked to high prices, which is not really\\nthe case in Paris. There are therefore important differences between the metropolises. This point\\ncan be verified by looking more closely at the correlations between the variables and the price per\\nm2by distinguishing between the metropolises. For the interquartile range, the correlation is much\\nhigher in Nice (0.41) than in Paris (0.27). These differences between cities justify the construction\\nof different models for each city.\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d6053ba-12a0-4540-899b-14ead141e9b6', embedding=None, metadata={'page_label': '18', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='(a) Interquartile range to median and price per m2,\\nNice\\n(b) Interquartile range to median and price per m2,\\nParis\\n3.3.3 Study of types of properties (houses and apartments)\\nHouses and apartments are very different properties. We look at some statistics to see this.\\nFigure 9: Distribution of surface and number of rooms by type of properties\\nThe surface area of the property and the number of rooms (which are correlated at -0.19 and\\n-0.22 respectively) differ greatly depending on the type of property. Separate models are therefore\\nmore appropriate to study these two different markets.\\nNext, we study the differences on these same variables between the metropolises (Figure 23 in ap-\\npendix). We observe that the surface area for houses is relatively different. This is a new argument\\nfor building different models.\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70d1a94d-3774-4238-a126-da8902f59366', embedding=None, metadata={'page_label': '19', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 10: Share of each properties in all metropoles\\nFinally, the distribution of properties between houses and apartments is relatively unequal be-\\ntween the metropolises. Including both types of property in the same model for each metropolis\\nwould make it more difficult, if not impossible, to compare the results between the metropolises.\\nIndeed, if our models estimate the prices of apartments well but less well those of houses, then the\\nestimates for Lille will be less good in general than those for Nice, even though we may estimate\\nthe apartments of both cities equally well. This could lead us to disqualify the model for Lille\\nwhereas a Lille-apartment model could remain interesting.\\n19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61407e25-8fc0-4052-8538-15fc7a996264', embedding=None, metadata={'page_label': '20', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3.4 Study of target variable\\nFigure 11: Distribution of price m2for each métropoles and house/apartments\\nWe now look at the target variable, prix_m2_actualise. We see that the distribution is very differ-\\nent from one metropole to another and can also be very different between houses and apartments\\ninside one metropole. This analysis supports our strategy to build one model for each couple\\nmetropole/house-apartment.\\nCleaning stepShare of transaction left\\nvs original datasetShare of transaction left\\nvs previous step\\nOriginal dataset100%\\n(18 036 812)\\nSelection of metropolis18%\\n(3 271 788)18%\\nCleaning on multivente7%\\n(1 269 091)39%\\nFiltering on surface and rooms6%\\n(1 041 960)82%\\nFiltering on prices6%\\n(1 007 243)97%\\nTable 1: Evolution of transacion number through cleaning process\\n20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e1c70ec-f1a2-4c7b-8a58-ee2218b7c969', embedding=None, metadata={'page_label': '21', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 Modeling\\n4.1 Preprocessing\\nBefore starting the modelling part of the project, it is necessary to make a selection of the data.\\nIndeed, for the models described in the following section to work at their best, the raw data must\\nbe processed.\\n4.1.1 The working features\\nAfter the cleaning and enrichment carried out in the previous section, we select the most significant\\nfeatures present in the database capable of characterising the price of a property.\\nTo predict the feature of interest, the price per m2of a property ( prix_m2 ), the database is\\ncomposed of 7 families of features :\\n•The price per m2of nearby neighbours ( prix_m2_zone )\\n•Property characteristics features such as the actual living space of the property in square\\nmeters and Number of main rooms ( surface_reelle_bati, nombre_pièces_principales )\\n•Nearbyamenitiesfeatures, theavailabilityoftheseamenitiescanaffectthevalueofaproperty\\n(Ecoles, Commerces, Gares, Banques, Medecins, Cinema,... )\\n•Features of the quality of education of nearby institutions. This is the average of the middle\\nschoolsforthebrevetandthehighschoolsfortheBaccalauréat. ( moyenne, moyenne_brevet )\\n•Geographical features, which define the location of the property. They can significantly\\ninfluence the price of a property. ( latitude, longitude, code_departement )\\n•Income features of the IRIS. These features give an indication of the wealth of the IRIS,\\nwhich as seen above impacts on the price of the property. ( mediane, D1, D9, Q3,... )\\n•Social features of the IRIS. These features characterise the population living in the IRIS.\\n(Part_retraite, Taux_pauvreté_seuil_60, Part_minima_sociaux, part_prestation_sociales,... )\\n4.1.2 Preprocessing\\nPre-processing techniques are an essential stamp in data analysis and machine learning. They\\nare used to prepare data for modeling by cleaning, transforming, and selecting relevant features.\\nThe following is an exhaustive list of common pre-processing techniques that we have used in our\\nproject:\\n21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99cbc83a-3532-4189-9d88-7cc19a45c204', embedding=None, metadata={'page_label': '22', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='•Encoding : converting categorical variables into numerical values so that it could be fitted\\nto a machine learning model. For this we apply one-hot encoding to all the categorical fea-\\ntures in our dataset. The only categorical variable used in the model is code_department\\ndue to its low cardinality.Moreover, we build a model by metropolis and use the average\\nprice of the neighbour as a feature, So much of the location information is already included\\nin this feature. Therefore it is not necessary to encode and use features like commune and\\nadresse_voie which have a high cardinality.\\n•Feature Selection : Used to select the most relevant features from a dataset to reduce di-\\nmensionality. Wefirstcalculatethecorrelationmatrix, andremovehighlycorrelatedcolumns\\nas well as unnecessary ones .Some features such as longitude andlatitudeare very correlated\\nmost of the time. In practice, for each metropolis and type of property, we assess the correla-\\ntion between each variable and we remove one of the two features highly correlated between\\nthem (over 95%).\\n•Standardization : A technique used to transform the data so that it has zero mean and\\nunit variance. This method helps to compare different variables on a common scale and thus\\nimproves the results of the model . In our case, we use ’StandardScaler’.\\nThe mathematical formula for StandardScaler can be expressed as:\\nz=x−u\\ns\\nwhere xis the input data, uis the mean of the input data, sis the standard deviation and\\nzis the transformed data after scaling.\\nWe have standardized all numerical columns in our model.\\n4.2 Split Train and test and Data leakage problem\\nData leakage in machine learning occurs when information from the training data is inadvertently\\nor intentionally included in the model, leading to biased or inaccurate results. There are several\\ntypes of data leakage that can occur in machine learning:\\n•Target leakage : This occurs when information that would not be available during the\\nprediction phase is used to train the model.\\n•Data snooping : This occurs when the model is trained using data that has been modified\\nor filtered based on the test set. This can result in overly optimistic performance metrics.\\n22', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf75ca57-eaa0-4290-b043-266233c25fb2', embedding=None, metadata={'page_label': '23', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='To avoid leakage problem in our modelling we take some precautions:\\n– Divide the data according to their geographical position into 10 major metropolises.\\n– For each metropolis split Train and test according to time . In fact we find the best trimester\\nto actualise our price so that we will have approximately 80% for the train set and 20% for\\nthe test set(’2021-T2’)(Table 2). So all data up to this trimester will be used as a test set\\nand the price will not be actualised for this set.\\n– All pre-processing transformations (imputation of missing data, standardization, encoding,\\noutlier detection) will be applied while maintaining independence between the training set\\nand the test set. A good practice is to define all our transformations in a pipeline.\\nMetropolis Train_sample Test_sample\\nMétropole du Grand Paris79,25%\\n(335 975)20,75%\\n(87 963)\\nMétropole Nice Côte d’Azur77,83%\\n(44 634)22,17%\\n(12 715)\\nMétropole d’Aix-Marseille-Provence87,78%\\n(96 827)12,22%\\n(13 485)\\nRennes Métropole79,02%\\n(27 866)20,98%\\n(7 400)\\nToulouse Métropole83,35%\\n(48 788)16,65%\\n(9 749)\\nBordeaux Métropole77,19%\\n(46 952)22,81%\\n(13 874)\\nMontpellier Méditerranée Métropole80,67%\\n(29 274)19,33%\\n(7 014)\\nNantes Métropole78,96%\\n(41 766)21,04%\\n(11 127)\\nMétropole Européenne de Lille78,49%\\n(64 171)21,51%\\n(17 586)\\nMétropole de Lyon81,33%\\n(73 263)18,67%\\n(16 814)\\nTable 2: Train set / test set distribution\\n4.3 The models\\nWe have decided to use two main families of models: Linear and ensemble learning models.\\n4.3.1 Linear Regression\\nLinear regression is a supervised learning algorithm used to establish a mathematical relationship\\nbetween features and make predictions for continuous or numeric features. The primary objective\\n23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cba9279f-81ca-40e0-951a-89f691eec7c7', embedding=None, metadata={'page_label': '24', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='of this algorithm is to find the best-fit line that describes the relationship between the features.\\nOne of the key benefits of linear regression is transparency and interpretability as it provides in-\\nsight into the relationship between input and output features unlike black-box models that may\\nproduce accurate predictions but lack transparency.\\nThe multiple linear regression model is the statistical tool most commonly used for the study of\\nmultidimensional data. As a particular case of a linear model, it represents the natural general-\\nization of simple regression.\\nThe multiple linear regression model can be mathematically represented as:\\nY=β0+β1X1+β2X2+· · ·+βpXp+ε (1)\\nWhere Yis the dependent feature, X1,X2, ...,Xpare the independent features, β0,β1,β2, ...,βp\\nare the regression coefficients, and εis the error term.\\nIn the context of retail, linear regression is used to predict house prices by analyzing the relation-\\nship between the price of a property and its various attributes. We decide to test this model as a\\nfirst approach, as it is regularly used in the literature on the subject. Moreover, as explained, it\\noffers a good interpretability of the results, which is important for MeilleurTaux.\\n4.3.2 The use of ensemble learning models to limit over-fitting\\nWhat is an ensemble learning model?\\nEnsemble learning models are based on a method that consists of combining a set of weak\\nlearners, learners that do not perform very well separately, often because of an over-fitting prob-\\nlem. That means, the generated algorithm is very sensitive to the training data, so the error\\nobtained on this sample will be very low but it will result in poor quality predictions on a new\\ndata set because the model will be too specialized to the train set. This combination of weak\\nlearners allows to increase the learning performance of the model and to obtain a higher level of\\naccuracy than the one that would be obtained by using separately one of these learners. Thus,\\nthese models have the advantage of limiting over-fitting by reducing the variance of the latter since\\nthe overall model is less sensitive to the training data. This is why the modeling of the project\\nheads toward the use of ensemble learning models as recommended by the state of the art on this\\nsubject. Indeed, the objective of this project is to obtain the prediction of a property according\\nto its characteristics. However, a property is rarely put up for sale several times over a short pe-\\nriod of time. So, it is important to limit the over-fitting of our model in order to obtain the most\\naccuratepricepredictionsoncethemodelisusedonpropertiesnotpresentinourtrainingdatabase.\\n24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6e5f26b-cf4c-47fd-9c68-9205f4aa7d95', embedding=None, metadata={'page_label': '25', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='We could distinguish two main ensemble learning methods, the parallel and the sequential. Both\\nwill be tested in this project in order to select the one that gives the best results for the problem.\\nBagging\\nWhat is bagging?\\nBagging is a method that consists in combining a set of weak learners in parallel and then keep-\\ning the majority prediction within this set. To do this, bagging relies on the statistical method of\\nBootstrap. Bootstrap draws uniformly and with discount observations from the starting data set\\nin order to create from it n different data sets. On each of these data sets, we randomly draw a\\nnumber d of features on which the learner will be based. We then train a learner on each data\\nset with x observations and d random features. Once the n different learners have been trained, a\\nprediction is made simply by taking a majority vote.\\nRandom Forest and decision tree\\nThe bagging method can be used on any type of predictive models to create weak learners.\\nWhen it is used on decision trees, this model is known as Random Forest, which will be tested in\\nthis project. Thus, to predict the price per m2 of a property, the model will be based on a set of\\ndecision trees.\\nRegression using a decision tree consists in predicting the value of a new observation according\\nto the leaf in which the algorithm will place this new property. To do this, the algorithm will\\nconstruct decision rules based on the characteristics of the property in order to identify its price.\\nThe algorithm will proceed by iteration. During the first iteration, it will separate the observations\\ninto K groups (in this project, K=2) using the variable allowing the best split in order to explain\\nthe target variable, here the price per m2. This split will create K sub-populations corresponding\\nto the first nodes of the tree. This splitting process will then be repeated several times on each\\ngroup creating new nodes until the process stops when the maximum depth of the tree is reached\\nor the regression cannot be improved. The result is the leaves of the tree corresponding to the\\naverage value of the price per m2 of the training observations in it.\\n25', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f4104fd-8448-4cc7-9644-0cbc115db1a7', embedding=None, metadata={'page_label': '26', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 12: Example of a decision tree for a property\\nBoosting\\nWhat is boosting?\\nBoosting is a sequential set method, meaning that the weak learners used in boosting will be\\nhighly dependent on each other. Unlike bagging, in which the learners are trained independently\\nof each other, in boosting, they are trained iteratively. Indeed, when the first regressor is trained,\\nall observations have an equal weight. From the results of this first model, if an observation is\\nbadly predicted, its weight increases. Thus, thanks to this new weighted data set, a new learner is\\ntrained to try to correct the errors of the previous learner. This training and error weighting pro-\\ncess continues until the entire train set is correctly predicted or the maximum number of models is\\nadded. The predictions of the last added model are then the global weighted predictions provided\\nby the previous set of models.\\nFigure 13: Differences in operation between ensemble learning models\\nsource:https://blent.ai/xgboost-tout-comprendre/\\n26', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f792e0a-8309-49ec-b6b3-d6d079fdbcef', embedding=None, metadata={'page_label': '27', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Gradient boosting\\nGradient boosting is a special case of boosting, in which the errors are minimized thanks to\\na gradient descent algorithm. Thus, only the first weak learner tries to predict the value of the\\nobservation. Indeed, the next models will try to predict the residuals of the observation from the\\nprevious model. That means, the difference between the predicted value and the real value of the\\nobservation. Moreover, Gradient boosting still respects the boosting principle of weighting the\\npreviously poorly predicted observations so that the new model concentrates its efforts on them.\\nFinally, the final prediction will be the weighted sum of all the generated models. In the same way\\nas the random forest, the weak learners used are decision trees, the principle of which has been\\nexplained previously.\\n4.4 Selection of the best parameters of models by cross-validation and\\nselection of the best models\\n4.4.1 K-fold Cross Validation and scoring Metrics\\nK-fold Cross-validation is a technique used to estimate the accuracy of a predictive model in the\\ncse of regression where we aim to make predictions.The main idea is to evaluate a model’s perfor-\\nmance on an independent dataset. Specifically, It works by partitioning the dataset into k-folds,\\ntraining in k-1 folds and then testing on the remaining fold. We then repeat the process k times,\\nwith each fold used once for testing. Finally, we average the results of each iteration in order to\\nobtain an overall estimate of model performance.\\nCross-validation is practically useful because it allows us to assess the generalizability of a\\nmodel. It also helps in selecting the optimal values for model parameters (hyperparameter tuning)\\nto control its performance.\\nThe mathematical formula for k-fold cross-validation is as follows:\\nCV(n, k) =1\\nkkX\\ni=1Li\\nwhere CV (n, k)represents the cross-validation estimate of model performance using k-folds, and\\nLirepresents the loss (or error) of the model on the i-th fold.\\nIn our case, we decide to use the R-squared metric to evaluate the performance of a model\\nand to compare it. R-squared, or the coefficient of determination, is a common metric used in re-\\n27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='787b6365-5c9c-4cd5-8f6b-717a8b9ed498', embedding=None, metadata={'page_label': '28', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='gression analysis to evaluate the performance of regression models. The key idea of this technique\\nis to measure the proportion of the variance in the dependent variable that is explained by the\\nindependent variables.\\nThe mathematical formula for R-squared is as follows:\\nR2= 1−SSres\\nSStot\\nWhere SSresrepresents the sum of squares of the residuals, which are the differences between the\\npredicted and actual values of the dependent variable :\\nSSres=nX\\ni=1(yi−ˆyi)2\\nWith yiis the actual value and ˆyiis the predicted value.\\nAndSStotrepresents the total sum of squares, which is the sum of the squared differences between\\nthe actual values and the mean of the dependent variable :\\nSStot=nX\\ni=1(yi−¯y)2\\nWith yiis the actual value and ¯yis the man value of the dependent variable.\\nIn general, the higher the R-squared, the better the model fits the data. In the context of\\ncross-validation, R-squared is a good metric because it measures the quality of the model fit to the\\ndata as well as a generalizability of the model which makes it a better model selection criterion\\nfor choosing between models.\\n4.4.2 Choice of the best model by type of property and metropole\\nFor each metropole , type of property and type of model (xgboost, random forest , Linear regres-\\nsion model), we performed a cross validation with a Grid search to find the best combination of\\nhyperparameters for our model. Then we choose from the 3 models the one with the best score on\\nthe cross-validation.\\n28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ea198a0-6fa3-4831-9857-031a87c2e776', embedding=None, metadata={'page_label': '29', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The choice of models is grouped in the tables below:\\nApartment\\nMetropolis Best Model Score R2\\nParis Xgboost 0,80\\nNice Xgboost 0,50\\nMarseille Xgboost 0,63\\nRennes Xgboost 0,64\\nToulouse Xgboost 0,65\\nBordeaux Xgboost 0,65\\nMontpellier Xgboost 0,60\\nNantes Xgboost 0,60\\nLille Xgboost 0,72\\nLyon Xgboost 0,68House\\nMetropolis Best Model Score R2\\nParis Xgboost 0,70\\nNice Random Forest 0,52\\nMarseille Xgboost 0,49\\nRennes Xgboost 0,66\\nToulouse Xgboost 0,48\\nBordeaux Xgboost 0,49\\nMontpellier Xgboost 0,36\\nNantes Xgboost 0,46\\nLille Xgboost 0,50\\nLyon Xgboost 0,48\\nIt can be seen that for the majority of the models xgboost gets the best scores. Moreover, the\\nscores are globally better for the apartment price prediction models.\\n5 Results\\nNow that we have trained models, it is important to evaluate them by analyzing their performance\\non the test data set, using different metrics to better understand the nature of their errors.\\n5.1 Evaluation\\n5.1.1 Metrics used\\nWe decided to focus on 3 different metrics for model evaluation: the Root-Mean-Squared-Error\\n(RMSE), theMean-Absolute-Percentage-Error (MAPE) and the Median-Absolute-Percentage-\\nError(MeAPE ). They were chosen to be evocative of the business impact that the models could\\nhave for maeilleurtaux.com\\n•RMSE: The formula of the Root-Mean-Squared-Error is:\\nvuut1\\nnnX\\ni=1(bYi−Yi)2 (2)\\nWe use this very classic Machine Learning metric because it is easily interpretable and\\npenalizes outliers. This is particularly relevant in the context of our mission because meilleur\\ntaux.com seeks to avoid making large estimation errors, which can be costly to their client.\\nIndeed, in the real estate sector, we can observe threshold effects on the amounts of the loans,\\n29', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='38c6925b-4248-4f70-8dd0-5f6a9ea96249', embedding=None, metadata={'page_label': '30', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='and it is more important to succeed in avoiding big estimation errors. The goal is to give an\\nidea of the price of the property, but not to estimate it perfectly. By using the Euclidean\\ndistance, the RMSE will then penalize these large deviations more strongly, which is why we\\nchose it.\\n•MAPE : The formula of the Mean-Absolute-Percentage-Error is:\\n100%\\nnnX\\ni=1\\x0c\\x0c\\x0c\\x0c\\x0cbYi−Yi\\nYi\\x0c\\x0c\\x0c\\x0c\\x0c(3)\\nMAPE is also very relevant to our project. Where the RMSE gives an idea of the \"raw\" error\\nof the estimate, the MAPE will allow us to obtain a metric calculated relative to the target\\nvariable. Indeed, it is not quite comparable to be wrong by 500 €on a price per square meter\\nof 1000 €and to be wrong by 500 €on a price per square meter of 4000 €. The MAPE allows\\nto calculate the average relative error made by the model, which will have a lot of added value\\nfor the business. This metric allows for example to compare the performance of the model\\nfor all clients equally, where the RMSE favors good performance for clients with high priced\\nproperties. MAPE allows for a comparable metric for a customer with a small apartment\\nand a customer with a large villa, which is important for Meilleurtaux in terms of fairness of\\ntreatment of their customers and inclusiveness. This is especially relevant since real estate\\nloans are generally assigned based on price indices. Therefore, this error can provide a good\\nestimate of the practical shortfall on a loan that a buyer could have obtained on the home\\nin question. So this metric does have a strong business meaning.\\n•MeAPE : The MeAPE is simply the calculation of the median of the relative errors rather\\nthan the mean as seen previously. We likewise study the median because the MAPE suffers\\nfrom some disadvantages related to its calculation method. First of all, the MAPE cannot be\\nused when the values are close to 0, but this is rarely the case in our study. However, MAPE\\nstill always suffers from a problem : imbalance. The MAPE is indeed not symmetrical and is\\nbiased towards high values, since the target variable is present in the denominator. In fact,\\nthe MAPE can exceed 100% error when the error is greater than twice the price per square\\nmeter. Therefore, we are studying the median as an alternative, which allows us to penalize\\nless the presence of outliers, which is natural while computing this error.\\n5.1.2 Performance assessment of the models on the test dataset\\nHere are the results of the models for each metric:\\n30', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00b83cb9-73db-42c8-9608-8f91d09ba590', embedding=None, metadata={'page_label': '31', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='•For apartments:\\nTable 3: Metropoles’ models evaluation on RMSE, MAPE and MeAPE, for apartments\\nMetropole RSME MeAPE MAPE\\nMétropole du Grand Paris 1464.8 9.74% 14.45%\\nMétropole d’Aix-Marseille-Provence 647.0 11.02% 15.37%\\nMétropole de Lyon 756.3 11.55% 14.32%\\nBordeaux Métropole 688.3 9.59% 13.12%\\nMétropole Européenne de Lille 556.4 10.87% 14.04%\\nMétropole Nice Côte d’Azur 898.0 12.02% 16.13%\\nRennes Métropole 637.613.20% 15.17%\\nToulouse Métropole 560.0 10.24% 13.31%\\nNantes Métropole 611.5 11.68% 13.99%\\nMontpellier Méditerranée Métropole 540.0 9.88% 12.87%\\n•For houses:\\nTable 4: Metropoles’ models evaluation on RMSE, MAPE and MeAPE, for houses\\nMetropole RSME MeAPE MAPE\\nMétropole du Grand Paris 1266.9 12.51% 16.05%\\nMétropole d’Aix-Marseille-Provence 790.911.30% 14.95%\\nMétropole de Lyon 992.9 13.40% 16.05%\\nBordeaux Métropole 848.7 11.77% 15.55%\\nMétropole Européenne de Lille 562.7 14.14% 17.67%\\nMétropole Nice Côte d’Azur 1446.4 17.31% 24.48%\\nRennes Métropole 648.2 12.14% 14.64%\\nToulouse Métropole 677.2 11.35% 15.14%\\nNantes Métropole 695.9 11.53% 13.64%\\nMontpellier Méditerranée Métropole 722.2 12.44% 15.26%\\nFirstly, it must be acknowledged that our results are conclusive. The MeAPE ranges from\\n9.59% to 13.20% for apartments, and from 11.30% to 17.30% for houses. It is normal for the\\naccuracy on houses to be slightly lower, as we have less data in our dataset. It is also normal for\\nthe MAPE to generally be higher than the MeAPE, because as we have mentioned, the nature of\\nthe formula used in the calculation pulls the errors upwards when the target variable is small.\\nWe also note that despite the fact that the RMSE for Paris is quite high, the errors themselves\\nare relatively small, as the price indices are just higher in Paris. Therefore, we can be satisfied\\nthat our model performs well in our largest cities (Paris, Marseille, Lyon...). We are even very\\nsuccessful for houses in Marseille, as well as apartments in Montpellier. Moreover our performance\\nis satisfactory across all cities, but we still notice a small weakness in Nice, where the price per\\nsquare meter is also generally high, but where we seem to capture certain factors less well than in\\n31', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7f6fce43-6796-4589-bc37-fc8abe9103fe', embedding=None, metadata={'page_label': '32', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Paris. This is probably due to the coastal nature of the city, which involves factors such as distance\\nto the beach or \"sea view\", which we did not take into account in the model. It would therefore\\nbe interesting to explore our model further to better understand the areas where it performs less\\nwell.\\n5.2 Interpretation\\nNow we can try to deep-dive our results to better understand our models and what are the drivers\\nthat lead them to make some mistakes.\\n5.2.1 Feature importance\\nFirstly, we can use the feature importance attribute of the model. Feature importance indicates\\nhow much each feature contributes to the model prediction. Essentially, it determines the degree of\\nusefulness of a specific variable for the current model and prediction. It is calculated by analyzing\\nthe weak estimators of the XgBoost model (often decision trees) and looking at the entropy gain\\nachieved by a split on this variable and the number of splits specific to this variable.\\nFigure 14: Feature Importance of the Bordeaux’ apartments model\\n32', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5d9ae26-d1b6-444e-9665-ce35226f3363', embedding=None, metadata={'page_label': '33', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 15: Feature Importance of the Bordeaux’ houses model\\nWe decided to take a closer look at the two models for Bordeaux, a city where we perform\\nwell. We notice, not surprisingly, that the price per square meter of the 10 closest neighbors\\nhas a significant impact on each model. We also observe this trend in the feature importance of\\nall cities: the price per square meter of the area has very high importance every time. As for\\nthe other variables, they are generally slightly more important for houses than for apartments,\\nindicating that the price per square meter of houses is less explained by that of the neighbors than\\nfor apartments.\\nSocio-economic indicators of the area often rank high, while amenities seem to have relatively\\nlow importance in all models. However, there is no clear hierarchy among the other variables,\\nwhose importance often varies depending on the city and type of housing.\\n5.2.2 Understanding of errors\\nTherefore, to truly understand what characterizes each model, we can try to focus on the nature\\nof the errors made by each one.\\n33', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70f7c268-cced-4847-8cca-59ebddb46069', embedding=None, metadata={'page_label': '34', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 16: Histogram of errors made by the Nice-Apartment model\\nThe models generally have error histograms similar to this one: a large majority of errors are\\nlessthan50%, thenthenumberoferrorsdecreasesrapidlybutwecontinuetomakebadpredictions,\\nsometimes up to 150%. An interesting question would be to ask if the properties that lead to these\\nhigh errors (>100%) have significantly different characteristics from other properties.\\n(a) Spider chart displaying the average\\ncharacteristics of two groups: individuals with low\\nerrors in blue (113316 individuals), individuals with\\nhigh errors in red (290253 individuals) for\\napartments\\n(b) Spider chart displaying the average\\ncharacteristics of two groups: individuals with low\\nerrors in blue (88290 individuals), individuals with\\nhigh errors in red (33171 individuals) for houses\\nWe notice that for apartments, the properties with high errors more often have a high price\\nper square meter of the neighborhood. This may be related to the fact that our model overfits the\\nprice of the nearest neighbors, and if this price is high but our property is in a worse condition, it\\n34', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3bcdda79-2b3f-4738-891f-7b0db82e4e87', embedding=None, metadata={'page_label': '35', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='can potentially cost much less, an effect not captured by our model which then overestimates the\\nprice, leading to larger errors.\\nOn the other hand, for houses, the profiles of the two groups are very similar: this shows\\nthat there is no fundamental difference between the two groups. This seems to suggest that our\\nmodels suffer from common variable biases and that we do not have enough indicators to effectively\\ndifferentiate properties with different prices per square meter.\\n(a) Spider chart displaying the average\\ncharacteristics of two groups: individuals with low\\nerrors in blue (22076 individuals), individuals with\\nhigh errors in red (11239 individuals) in Nice\\n(b) Spider chart displaying the average\\ncharacteristics of two groups: individuals with low\\nerrors in blue (13722 individuals), individuals with\\nhigh errors in red (4662 individuals) in\\nMontpellier\\nWe also observe this phenomenon for cities, where we can see that the profiles between the two\\ngroups are generally very similar. Despite a few cities where differences are observed, most curves\\nare very close, which once again suggests that the errors are the result of unexplained factors\\nrather than directly related to a modeling or variable error in our model.There are still some cities\\nwhere significant differences seem to emerge, such as Montpellier, where certain variables such as\\nthe actual surface area of the building, the number of main rooms or the number of lots seem\\nto have an effect on the model’s strong errors. We could then try to display these variables by\\nIRIS to try to understand if it is a geographical specificity, an error in the processing of variables,\\nan inconsistency in the base dataset...However, in most cities, such as Nice where we still make\\na number of large errors, there is no significant trend that seems to emerge between the groups,\\nshowingthatourvariablesarestillincompleteincapturingthefullrealityoftherealestatemarkets\\nin each metropolis.\\nNow that we have a better understanding of the model’s errors, an important question remains:\\nwill our models be able to perform well when we don’t provide them with a discount rate? In other\\nwords, will our models continue to be as accurate when deployed in the real world?\\n35', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05e5c2ad-b593-4e71-8001-2415308b9fd4', embedding=None, metadata={'page_label': '36', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Robustness of models over time\\nAs previously explained in the constraints of this project, temporality plays an important role.\\nMeilleur Taux wishes to obtain a model that is as robust as possible to time. We therefore decide\\nto analyze the performance of our model over time. To do this, we compare the predictions of the\\nmodels with these same predictions updated. Indeed, the current models predict the price per m2\\nof a property in the second quarter of 2021. So we update these prices to their real quarter of sale.\\nTo do this, we use the technique explained at the beginning of this report.\\nThe results obtained for each metropolis are presented in the table below:\\nApartment\\nMetropole diff MeAPE diff MAPE\\nParis 0,04% -0,04%\\nNice 0,22% 0,80%\\nMarseille 0,33% 0,86%\\nRennes -2,41% -1,40%\\nToulouse 0,16% 0,99%\\nBordeaux 1,26% 1,97%\\nMontpellier 0,07% 0,21%\\nNantes -1,16% -0,30%\\nLille 0,07% 0,01%\\nLyon 0,05% 0,02%\\nMean -0,13% 0,31%House\\nMetropole diff MeAPE diff MAPE\\nParis -0,24% 0,02%\\nNice -0,30% 0,68%\\nMarseille -0,09% 1,16%\\nRennes -1,48% -0,70%\\nToulouse 0,73% 1,04%\\nBordeaux -0,05% 1,13%\\nMontpellier -1,08% -0,27%\\nNantes -1,11% -0,08%\\nLille -0,55% 0,37%\\nLyon -0,92% -0,45%\\nMean -0,51% 0,29%\\nWe notice that for all the metropolises and all the properties (apartments and houses) discount-\\ning does not significantly improve the results. The differences between the results are minimal.\\nIndeed, on average, for apartments, updating reduces the median percentage error by only 0.13%\\nand by 0.51% in the case of houses. On the contrary, the discounting increases the median of\\nthe percentage of error by 0.3%. We conclude that the model seems to be robust to the near\\ntemporality. Indeed, the tested data ran for one year after the update date of the models train set\\n(Q2-2021). Thus, the requirement of robustness of the predictions to the temporality required by\\nMeilleur Taux is correctly satisfied.\\n36', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d2a6057-242c-4569-834a-33cb19a28ae8', embedding=None, metadata={'page_label': '37', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Conclusion\\nA look back to our objectives\\nOur work provides a ready to use tool to predict real estate price in the most important french\\nmetropolises. We have payed attention to the main challenges we had identified. The temporal\\ncomponent has been handled with price actualisation. We have tackled data leakage challenge with\\nrigorous train test split and preprocessing taking into account the temporal dimension of train and\\ntest data. Real estate being all about location we have splited our work on the metropolises. This\\npart could be improved by adding some variables specific to the metropolises. This geographical\\nsplit implies that we have inequal results through metropolises but this also allows us to take the\\nmost of some information in metropolises. We therefore have some very good results for some of\\nthem. Future improvements have a solid basis for development.\\nLimits and potential improvements\\nWe have identified few limits and possible improvement to our project.\\n•We could have benefited from some aditional information about the properties. This would\\ninclude for example : number of bedrooms, number of bathrooms, information about energy\\nconsumption of the property (energy class A-G), state of the property or date since last work.\\nOther variables could be more specialy linked to each metropolis. For example the sea view\\nfor Nice. This would potentially help reduce overfitting on price_zone .\\n•We have used random gridsearch so we don’t test for all hyperparameters. Maybe the best\\nones have not been tested.\\n•We could have refined our work on the price_zone for example by adding some informations\\nabout the distance.\\n•The price actualisation indices is based on an quite blurred methodology. Moreover, only\\nfour metropolis have their own indices. For all the others, the indice is less precise.\\nProject utilization\\nThe project was transformed into a package comprising various modules, namely pre-processing,\\nmachinelearning, andEDA.Thesemodulesareinvokedfromamainfunctionandgenerateoutputs\\nsuch as data, plots, metrics, and models, which are stored in a local folder. You can find the project\\nonGitHub at the following link.For instructions on how to use the project, please refer to the\\nREADME.md file. Additionally, the project also offers extensive documentation for all modules,\\nthe link to which is provided in the README.\\n37', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='84c4b073-295b-4a68-9e88-8aed18b2b07e', embedding=None, metadata={'page_label': '38', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Bibliography\\n1. Fagereng, Blomhoff Holm, Torstensen, \"Housing Wealth in Norway 1993-2015\" - 2020.\\n2. Joshua H. Gallin, Raven Molloy, Eric Nielsen, Paul Smith, andKamila Sommer, \"Measuring\\nAggregate Housing Wealth: New Insights from anAutomated Valuation Model\" - 2018.\\n3. Céline Grislain-Letrémy, Arthur Katossky, \"he impact of hazardous industrial facilities on\\nhousing prices: Acomparison of parametric and semiparametric hedonic price models\" - 2014.\\n4. Katharina Knoll, Moritz Schularick, Thomas Steger, \"No Price Like Home: Global House\\nPrices, 1870–2012\" - 2017.\\n5. Sendhil Mullainathan, Jann Spiess, \"Machine Learning: An Applied Econometric Approach\"\\n- 2017.\\n6. Mathilde Poulhes, \"From Latin Quarter to Montmartre Investigating Parisian Real-Estate\\nPrices\"- 2017.\\n7. Quang Truong, Minh Nguyen, Hy Dang, Bo Mei, \"Housing Price Prediction via Improved\\nMachine Learning Techniques\" - 2020.\\n8. Arnaud Hureaux- Projet : Estimateur de prix d’un bien immobilier basé sur du Machine\\nLearning :\\nhttps://hureauxarnaud.medium.com/projet-estimateur-de-prix-dun-bien-immobil\\nier-base-sur-du-machine-learning-ae578fdacaca/ .\\n9. Nada Belaidi- XGBoost : Tout savoir sur le Boosting :\\nhttps://blent.ai/xgboost-tout-comprendre/ .\\n10. Vianney Perchet, \"Theoretical fundations of Machine Learning\" (Cours de deuxième année\\nde l’ENSAE).\\n38', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9fb1361b-3557-4be6-8494-cea5e92f193f', embedding=None, metadata={'page_label': '39', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appendix\\nFigure 19: Feature correlation with prix_ m2_actualise Grand Paris Métropole\\n39', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4904f5ab-4ff8-4698-8130-1c91739cdc84', embedding=None, metadata={'page_label': '40', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 20: Feature correlation with prix_ m2_actualise Rennes Métropole\\n40', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='14cfdb29-0d94-4f0c-b5ce-dc4a47672978', embedding=None, metadata={'page_label': '41', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 21: Distribution of surface and number of rooms by type of properties, by metropole\\n41', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='855a94b5-4e38-4c60-bba5-5afed3656076', embedding=None, metadata={'page_label': '42', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 22: Documentation page of ’Discount’ module\\n42', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b93db2f0-285e-4e22-b84a-a6bb023b3f6a', embedding=None, metadata={'page_label': '43', 'file_name': 'Business_data_challenge_report_group_4.pdf', 'file_path': '/home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf', 'file_type': 'application/pdf', 'file_size': 9992974, 'creation_date': '2024-03-10', 'last_modified_date': '2024-03-10'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 23: Documentation page of ’Core’ module\\n43', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document =  SimpleDirectoryReader('content/').load_data() ## read my document. It is an article about XAI\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "You are a Q&A assistant. Your goal is to answer questions as\n",
    "accurately as possible based on the instructions and context provided.\n",
    "\"\"\"\n",
    "## Default format supportable by LLama2\n",
    "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b034b690884452d8fbb37559e2090b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"google/gemma-2b\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    device_map=\"auto\",\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import ServiceContext\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-langchain in /opt/mamba/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-embeddings-langchain) (0.10.18.post1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/mamba/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.13.3)\n",
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/mamba/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/mamba/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/mamba/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.6.3)\n",
      "Requirement already satisfied: anyio in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.4)\n",
      "Requirement already satisfied: idna in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/mamba/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/mamba/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/mamba/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/mamba/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/mamba/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/mamba/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/mamba/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/mamba/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/mamba/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-embeddings-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47750/2250285859.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context=ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "service_context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7efed8d686d0>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7efed8d686d0>, id_func=<function default_id_func at 0x7efee036d6c0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.core.service_context_elements.llama_logger.LlamaLogger object at 0x7f0137c8c290>, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7efed8d686d0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=document, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7f0137cee190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query('What is the goal of the business data challenge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The goal of the business data challenge is to provide a tool for predicting the price of real\n",
      "estate according to its characteristics (type of property, location, surface area,...). This tool\n",
      "made available to customers of Meilleurtaux would allow them to estimate the real estate value\n",
      "of their dwelling. Thus, this estimate is useful in two cases:\n",
      "•In a mortgage loan project, when you need a bridging loan.\n",
      "•In a credit consolidation project, when you have a mortgage guarantee.\n",
      "---------------------\n",
      "page_label: 43\n",
      "file_path: /home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf\n",
      "\n",
      "43\n",
      "\n",
      "page_label: 3\n",
      "file_path: /home/onyxia/work/LLM_RAG/content/Business_data_challenge_report_group_4.pdf\n",
      "\n",
      "4\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What is the goal of the business data challenge?\n",
      "Answer: <|ASSISTANT|>\n",
      "The goal of the business data challenge is to provide a tool for predicting the price of real\n",
      "estate according to its characteristics (type of property\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
